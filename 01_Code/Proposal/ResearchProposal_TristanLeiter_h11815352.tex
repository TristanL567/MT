%==== KNITR ===================================================================%



%==== START ===================================================================%

\documentclass{report}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[left=2cm, right=2cm, top=1cm, bottom=2cm]{geometry}

% Font.


% Main packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs} 
\usepackage{rotating} 
\usepackage{lmodern}

% Citations.
\usepackage{natbib}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands

%% Testing different layout.
\usepackage{geometry}
\usepackage{xcolor}

%% Grafik-Pfad.
\graphicspath{
    {C:/Users/TristanLeiter/Documents/Privat/MT/04_Charts/DescriptiveStatistics/}
}

% \title{Master Thesis - Research Proposal}
% \author{Tristan Leiter}
% \date{\today}

%==== DOCUMENT START ==========================================================%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{titlepage}
    \newgeometry{top=3cm, bottom=3cm, left=2.5cm, right=2.5cm}
    \centering
    
    % --- University Placeholder ---
    % \includegraphics[width=0.4\textwidth]{logo.png} \par
    {\scshape\LARGE Vienna University of Economics and Business \par}
    \vspace{1cm}
    {\scshape\Large Master Thesis Research Proposal \par}
    
    \vspace{2.5cm}
    
    % --- Title Section ---
    \hrule height 2pt
    \vspace{0.5cm}
    { \huge \bfseries The Agony and the Ecstasy: \\[0.3cm] Constructing a "Crash-Filtered" Equity Index \\[0.2cm] using Machine Learning \par}
    \vspace{0.5cm}
    \hrule height 2pt
    
    \vspace{2cm}
    
    % --- Author Section ---
    \Large
    \textbf{Author:} \\
    Tristan \textsc{Leiter}
    
    \vspace{1.5cm}
    
    % --- Metadata ---
    \normalsize
    \textbf{Submission Date:} \\
    \today
    
    \vspace{1cm}
    
    \textbf{Supervisor:} \\
    Univ.Prof. Dr. Kurt Hornik \\
    Department of Finance, Accounting and Statistics
    
    \vfill
    
\end{titlepage}
\restoregeometry

% \maketitle

%==== ABSTRACT ================================================================%

% \begin{abstract}
% This report demonstrates the integration of R code and its output within a LaTeX document using Sweave. It covers the basic structure of a report, including a summary, chapters with subchapters, and a bibliography.
% \end{abstract}

%==== Table of content ========================================================%

% \tableofcontents
% \newpage

%==== Chapter 1: Problem Description ==========================================%

\chapter{Problem Description}

\section{Introduction}

% The theoretical foundation of modern portfolio management relies on the diversification of idiosyncratic risk. Empirical evidence suggests that the distribution of individual stock returns is not normally distributed, but is instead characterized by extreme skewness and "fat tails". \\

Research by J.P. Morgan Asset Management highlights an empirical phenomenon referred to as "The Agony and the Ecstasy" \citep{Cembalest2014, Cembalest2024} revealing that equity indices, like the Russell 3000, are overwhelmingly influenced by extreme stock performances. While a small percentage of winners contribute the vast majority of excess returns, approximately 40\% of all constituents suffer a "catastrophic decline," which is defined as a drawdown of 70\% or more from their peak without a subsequent recovery. \\

Recent literature formalizes this phenomenon as a \textbf{"Catastrophic Stock Implosion"} (CSI) \citep{Tewari2024}. Unlike standard volatility, an implosion represents a distinct market event characterized by a severe price downturn followed by prolonged stagnation and minimal probability of recovery. This presents a critical challenge for index construction: passive investing captures the "Ecstasy" of winners but systematically forces investors to hold the "Agony" of these imploding assets (at least until removal from the constituent index). Crucially, as explored by \citet{Cembalest2014, Cembalest2024}, these declines are not limited to speculative "junk" companies; a significant portion of losers display "healthy" fundamentals prior to collapse, suggesting traditional metrics fail to capture the dynamics preceding permanent capital decline.

\section{Status Quo}

\begin{enumerate}
    \item \textbf{The Quality Trap:}  Recent literature has explored indicators which are perceived to be synonymous with healthy stock fundamentals. \citet{Penman2018} suggest that the Book-to-Price ratio (B/P) is misleading. Low B/P values reflect uncertainty about future cash-flows rather than
"cheap" buying opportunities. Additionally, \citet{Altman2016} argue that, amongst others, profitability is time-varying and find low predictive ability for longer time-periods. Specifically, while they find that profitability ratios like Return on Assets (ROA) provide efficient accuracy for a short horizon of two years , these measures fail to be consistent predictors over a ten-year horizon. They observe that in multivariate models, profitability is rendered largely insignificant when tested against solvency measures, such as the Equity Ratio, which dominates the prediction of distress irrespective of the horizon length.

  \item \textbf{Predictive Ability:} Traditional bankruptcy models heavily relied on linear combinations of ratios, like the Z-score introduced by \citep{Altman1968}. Since then, a variety of
risk-management models have been introduced, culminating in modern ML applications, such as
Random Forest (RF) or support vector machines \citep{Barboza2017}. Even though logit/probit models
work reasonably well, \citet{Jones2017} also recommends the use of more advanced machine-learning
methods, like AdaBoost or RF. \\
  
\item \textbf{Misalignment of Prediction Horizons and Objectives:} 
While traditional bankruptcy models aim to minimize credit risk, they often fail to minimize market risk. A fundamental disconnect exists between \textit{legal insolvency} and \textit{market implosion}.

\begin{itemize}
    \item \textbf{Lagging Indicators:} Legal bankruptcy is frequently the final stage of a long deterioration process. By the time a traditional Altman Z-Score or structural model flags a company as distressed, the market has often already priced in the failure, resulting in a "Zombie state" where the asset lingers at depressed valuations \citep{Tewari2024}. For an equity investor, the capital is lost at the \textit{implosion} event, not the bankruptcy filing.
    \item \textbf{The Cost of False Positives (Type I Errors):} In the context of equity indexing, the cost of a False Positive is mainly the opportunity cost. As noted in the "Agony and Ecstasy" framework, index returns are driven by a small tail of extreme winners. Traditional models, which penalize negative skewness too aggressively, risk flagging volatile but successful growth stocks as "distressed." Excluding a future "Ecstasy" stock (like NVIDIA) due to a conservative model, which produces too many False Positives, would severely underperform the benchmark, negating the benefits of avoiding the "Agony" stocks.
\end{itemize}

\end{enumerate}

\section{Research Gap and Proposed Methodology}

\subsection{Implications of Current Limitations}
 
\citet{Jabeur2021, Jabeur2023} have demonstrated that ML models, such as Gradient Boosting and Neural Networks,
outperform statistical models in predicting financial distress by capturing non-linear relationships between variables. \citet{Tewari2024} have built on these insights and applied modern ML-techniques to market-based risk-modelling, for which they find that XGBoost can predict up to 61\% of implosions in the test set
with a false positive rate of less than 3\%. \\

While the literature has identified a variety of ML-methods for credit-risk modelling, 
there is a lack of research on applying modern ML techniques to the specific problem of 
 \textit{market-based} catastrophic stock declines. Ensemble methods, like bagging and boosting,
 or Neural Networks are well suited to not only improve predictive accuracy, but also to 
 capture the time-series dynamics of individual stock risk. \\

% \begin{enumerate}
%     \item \textbf{From Ratios to Raw Data:} By utilizing raw financial data rather than derivative ratios, Machine Learning (ML) algorithms can autonomously learn complex, non-linear dependencies and interaction effects that traditional ratios (like Book-to-Price) fail to capture.
%     \item \textbf{From Static to Dynamic:} Financial distress is rarely a static state but a sequence of deteriorating signals. Traditional cross-sectional models ignore the temporal trajectory of firm fundamentals.
% \end{enumerate}

\subsection{Proposed Approach: The "Crash-Filtered" Index}

This thesis proposes bridging the gap between distress prediction and active index construction. While \citet{Tewari2024} established the concept of Catastrophic Stock Implosion (CSI), it remains unknown whether these insights can be operationalized into a viable risk-mitigation strategy. To address the "Agony and Ecstasy" dilemma, this research moves from "pure volatility forecasting" to "probabilistic implosion modeling" through the following three-stage methodology:

\begin{enumerate}
    \item \textbf{Model Fitting and Selection:} \\
    The research will model the probability of market-risk loss, specifically the CSI event, by adopting the definition established by \citet{Tewari2024}. The methodology extends current literature by moving beyond static snapshots of financial health. In addition to advanced ensemble methods (e.g., XGBoost, CatBoost) capable of handling non-linear interactions, this approach incorporates autoencoders to decode the noisy financial features.
    
% This extension allows the model to capture \textit{latent temporal dependencies} and the rate of change in fundamental deterioration, aiming to reduce the detection lag associated with traditional models.

    \item \textbf{Index Construction:} \\
    Based on the model outputs, a "Crash-Filtered" equity index will be constructed. This process involves a systematic re-weighting or exclusion mechanism that penalizes index constituents exhibiting a likelihood of implosion exceeding a calibrated threshold. The primary objective is to penalize identified "Agony" candidates (permanent capital loss or capital loss exceeding a threshold) while retaining the "Ecstasy" winners (usually featuring high volatility and returns). This distinction is critical to isolating the alpha generated purely by tail-risk mitigation, rather than by a generic low-beta factor tilt.

    \item \textbf{Backtesting and Performance Evaluation:} \\
    The efficacy of the strategy will be validated through out-of-sample backtesting using a rolling-forward cross-validation framework. The performance of the "Crash-Filtered" index will be benchmarked against:
    \begin{itemize}
        \item The unfiltered market-weighted benchmark (e.g., Russell 3000) to test for alpha generation.
        \item Traditional volatility-weighted portfolios (e.g., Minimum Volatility indices) to test for superior drawdown characteristics.
    \end{itemize}
    Evaluation metrics will prioritize risk-adjusted returns and tail-risk characteristics.
\end{enumerate}


%==== Chapter 2: Research Question ============================================%

\chapter{Research Question}

Based on the identified problem that standard metrics fail to distinguish between "recoverable volatility" and "permanent implosion" and that perceived quality-signals can be misleading, the following research question could be explored:

\section{Main Research Question}

\textit{To what extent does a 'Crash-Filtered' equity index, constructed using a hybrid of Autoencoder-based feature extraction and Gradient Boosting Ensembles, generate superior risk-adjusted returns compared to traditional volatility-weighted strategies?}

\subsection{Sub-Question 1: Autoencoder}

Standard ratios (like P/E) are too noisy, but an Autoencoder can find the "hidden" structure of a failing firm.\\

\textit{Does the integration of latent features derived from Autoencoders significantly improve the predictive accuracy (AUC-ROC) of Gradient Boosting models compared to using raw financial ratios alone?} \\

\textit{Hypothesis:} Autoencoders will successfully denoise volatile market indicators, allowing the Ensemble model to identify "structural" distress earlier than models relying on raw accounting inputs.

\subsection{Sub-Question 2: The "False Positive" Advantage of Ensemble Methods}

Showing that decision trees (handling the non-linearities) are better than the linear Altman Z-Score at saving the "Ecstasy" stocks.

\textit{Do Ensemble methods (e.g., CatBoost, XGBoost) exhibit a statistically lower Type I error rate (False Positives) than linear discriminant models (Altman Z-Score) when classifying high-volatility growth stocks?} \\ 

\textit{Hypothesis:} Ensemble models will distinguish between 'good' volatility (growth) and 'bad' volatility (implosion) more effectively than linear models, thereby reducing the exclusion of high-performing winners.

\subsection{Sub-Question 3: The "Tail-Risk" Advantage}

\textit{Can a portfolio filtered by this hybrid ML-probability score deliver superior downside protection (lower Maximum Drawdown) than a Minimum Volatility benchmark, without sacrificing the upside participation of a market-cap weighted index?} \\ 

\textit{Hypothesis:} The Hybrid-Filtered Index will decouple downside risk from upside potential, generating alpha specifically through the avoidance of Catastrophic Stock Implosions.

%==== Chapter 3: Research Design ==============================================%

% \chapter{Research Design}
% \label{ch:research_design}
% 
% To answer the research question posed in Chapter 2, this thesis employs a quantitative, three-stage methodology. The approach transitions from unsupervised feature extraction to supervised probabilistic modeling, culminating in the construction of a systematic investment strategy. The methodology is designed to test whether machine learning can effectively separate "recoverable volatility" from "permanent capital loss" (Cembalest, 2014; Tewari et al., 2024).
% 
% \section{Data and Sample Selection}
% The research universe consists of the constituents of the \textbf{Russell 3000} index, representing approximately 98\% of the investable U.S. equity market. This broad universe is essential to capture both large-cap stability and the "long tail" of small-cap volatility where implosions are most frequent.
% 
% \subsection{Data Sources}
% \begin{itemize}
%     \item \textbf{Market Data:} Daily adjusted closing prices and trading volume (Source: CRSP/Bloomberg).
%     \item \textbf{Fundamental Data:} Quarterly accounting ratios including leverage, liquidity, and profitability metrics (Source: Compustat).
%     \item \textbf{Macroeconomic Indicators:} VIX index, interest rates, and credit spreads to control for systematic market stress.
% \end{itemize}
% 
% \subsection{Defining the Target Variable: Catastrophic Stock Implosion (CSI)}
% Following the definition established by Tewari et al. (2024)[cite_start], a stock is classified as experiencing a Catastrophic Stock Implosion (CSI) if it suffers a drawdown of $D \geq 70\%$ from its trailing peak without recovering to at least 50\% of the peak value within a subsequent 12-month period[cite: 15, 16].
% 
% Let $P_t$ be the price at time $t$ and $P_{peak}$ be the rolling 2-year maximum price. The binary target variable $y_{i,t}$ for stock $i$ is defined as:
% \begin{equation}
%     y_{i,t} =
%     \begin{cases}
%     1 & \text{if } \frac{P_t - P_{peak}}{P_{peak}} \leq -0.70 \text{ and No Recovery} \\
%     0 & \text{otherwise}
%     \end{cases}
% \end{equation}
% 
% \section{Methodology Pipeline}
% The research design follows a sequential "Hybrid" pipeline:
% 
% \begin{enumerate}
%     \item \textbf{Unsupervised Phase:} An Autoencoder compresses high-dimensional, noisy financial data into latent features.
%     \item \textbf{Supervised Phase:} Gradient Boosting Ensembles (XGBoost/CatBoost) predict the probability of CSI using these latent features.
%     \item \textbf{Portfolio Phase:} A "Crash-Filtered" index is constructed by removing stocks with a predicted CSI probability above a dynamic threshold.
% \end{enumerate}
% 
% \section{Stage 1: Feature Extraction via Autoencoders}
% Financial ratios (e.g., P/E, Debt/Equity) are often noisy and exhibit high multicollinearity. To address this, an undercomplete \textbf{Autoencoder} (a type of Neural Network) is employed to learn a compressed representation of the input space.
% 
% The Autoencoder maps the input vector $x$ to a latent representation $z$ (encoding) and attempts to reconstruct the input $\hat{x}$ (decoding). The objective is to minimize the reconstruction error:
% \begin{equation}
%     \mathcal{L}(x, \hat{x}) = || x - \hat{x} ||^2
% \end{equation}
% The resulting latent vector $z$ serves as a "denoised" feature set for the subsequent classification model, aiming to capture structural distress signals that linear ratios might miss.
% 
% \section{Stage 2: Predictive Modeling (Ensemble Methods)}
% The core predictive engine utilizes \textbf{Gradient Boosted Decision Trees (GBDT)}, specifically \textbf{XGBoost} and \textbf{CatBoost}. These models are selected for their ability to:
% \begin{itemize}
%     [cite_start]\item Handle non-linear interactions between variables[cite: 40].
%     \item Process missing data inherently.
%     [cite_start]\item Minimize the False Positive rate, which is critical to retaining high-volatility "winners"[cite: 37].
% \end{itemize}
% 
% \subsection{Benchmarking Prediction}
% To validate the hypothesis that ML outperforms traditional metrics, the Ensemble model's performance will be compared against:
% \begin{itemize}
%     [cite_start]\item \textbf{Baseline 1:} Altman Z-Score (Linear Discriminant Analysis)[cite: 26].
%     \item \textbf{Baseline 2:} Logistic Regression.
% \end{itemize}
% Evaluation metrics will focus on the \textbf{Area Under the Precision-Recall Curve (PR-AUC)} due to the high class imbalance of implosion events.
% 
% \section{Stage 3: Index Construction and Backtesting}
% The final stage involves constructing the "Crash-Filtered" index. The portfolio is rebalanced monthly.
% 
% \subsection{The Filtering Mechanism}
% For each rebalancing period $t$:
% \begin{enumerate}
%     \item Calculate the probability of implosion $\hat{p}_{i,t}$ for all constituents using the trained Ensemble model.
%     \item Exclude any stock where $\hat{p}_{i,t} > \theta$, where $\theta$ is a calibrated risk threshold (e.g., 50\%).
%     \item Re-weight the remaining constituents according to market capitalization.
% \end{enumerate}
% 
% \subsection{Performance Evaluation}
% [cite_start]The strategy is backtested using a rolling-forward (walk-forward) cross-validation framework to prevent data leakage[cite: 59]. The "Crash-Filtered" Index is benchmarked against:
% \begin{itemize}
%     \item \textbf{The Unfiltered Benchmark:} Russell 3000 (Market-Cap Weighted).
%     [cite_start]\item \textbf{The Volatility Benchmark:} MSCI USA Minimum Volatility Index (to test for distinctness from low-beta factors)[cite: 62].
% \end{itemize}
% 
% Key performance indicators include the \textbf{Sharpe Ratio}, \textbf{Calmar Ratio}, and \textbf{Maximum Drawdown}.
% 





%==== Chapter 4: Expected Results =============================================%

% \chapter{Expected Results}
% We expect to validate the findings regarding distress risk discussed by \citet{Campbell2005}.


%==== Chapter 5: Methodology ==================================================%



%==== Chapter 6: Research Background ==========================================%



%==== Chapter 7: References ===================================================%

\bibliographystyle{plainnat} 
\bibliography{references}

%==== Appendix ================================================================%

% \appendix 
% 
% \chapter{Overview}

%==== END =====================================================================%

\end{document}
