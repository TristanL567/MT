%==== KNITR ===================================================================%



%==== START ===================================================================%

\documentclass{report}\usepackage[]{graphicx}\usepackage[table]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[left=2cm, right=2cm, top=1cm, bottom=2cm]{geometry}

% Font.


% Main packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs} 
\usepackage{rotating} 
\usepackage{lmodern}

% Citations.
\usepackage{natbib}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands

%% Testing different layout.
\usepackage{geometry}
\usepackage{xcolor}
\usepackage[table]{xcolor}

%% Grafik-Pfad.
\graphicspath{
    {C:/Users/TristanLeiter/Documents/Privat/MT/04_Charts/DescriptiveStatistics/}
}

% \title{Master Thesis - Research Proposal}
% \author{Tristan Leiter}
% \date{\today}

%==== DOCUMENT START ==========================================================%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{titlepage}
    \newgeometry{top=3cm, bottom=3cm, left=2.5cm, right=2.5cm}
    \centering
    
    % --- University Placeholder ---
    % \includegraphics[width=0.4\textwidth]{logo.png} \par
    {\scshape\LARGE Vienna University of Economics and Business \par}
    \vspace{1cm}
    {\scshape\Large Master Thesis Research Proposal \par}
    
    \vspace{2.5cm}
    
    % --- Title Section ---
    \hrule height 2pt
    \vspace{0.5cm}
    { \huge \bfseries The Agony and the Ecstasy: \\[0.3cm] Constructing a "Crash-Filtered" Equity Index \\[0.2cm] using Machine Learning \par}
    \vspace{0.5cm}
    \hrule height 2pt
    
    \vspace{2cm}
    
    % --- Author Section ---
    \Large
    \textbf{Author:} \\
    Tristan \textsc{Leiter}
    
    \vspace{1.5cm}
    
    % --- Metadata ---
    \normalsize
    \textbf{Submission Date:} \\
    \today
    
    \vspace{1cm}
    
    \textbf{Supervisor:} \\
    Univ.Prof. Dr. Kurt Hornik \\
    Department of Finance, Accounting and Statistics
    
    \vfill
    
\end{titlepage}
\restoregeometry

% \maketitle

%==== ABSTRACT ================================================================%

% \begin{abstract}
% This report demonstrates the integration of R code and its output within a LaTeX document using Sweave. It covers the basic structure of a report, including a summary, chapters with subchapters, and a bibliography.
% \end{abstract}

%==== Table of content ========================================================%

% \tableofcontents
% \newpage

%==== Chapter 1: Problem Description ==========================================%

\chapter{Problem Description}

\section{Introduction}

% The theoretical foundation of modern portfolio management relies on the diversification of idiosyncratic risk. Empirical evidence suggests that the distribution of individual stock returns is not normally distributed, but is instead characterized by extreme skewness and "fat tails". \\

Research by J.P. Morgan Asset Management highlights an empirical phenomenon referred to as "The Agony and the Ecstasy" \citep{Cembalest2014, Cembalest2024} revealing that equity indices, like the Russell 3000, are overwhelmingly influenced by extreme stock performances. While a small percentage of winners contribute the vast majority of excess returns, approximately 40\% of all constituents suffer a "catastrophic decline," which is defined as a drawdown of 70\% or more from their peak without a subsequent recovery. \\

Recent literature formalizes this phenomenon as a \textbf{"Catastrophic Stock Implosion"} (CSI) \citep{Tewari2024}. Unlike standard volatility, an implosion represents a distinct market event characterized by a severe price downturn followed by prolonged stagnation and minimal probability of recovery. This presents a critical challenge for index construction: passive investing captures the "Ecstasy" of winners but systematically forces investors to hold the "Agony" of these imploding assets (at least until removal from the constituent index). Crucially, as explored by \citet{Cembalest2014, Cembalest2024}, these declines are not limited to speculative "junk" companies; a significant portion of losers display "healthy" fundamentals prior to collapse, suggesting traditional metrics fail to capture the dynamics preceding permanent capital decline.

\section{Status Quo}

\begin{enumerate}
    \item \textbf{The Quality Trap:}  Recent literature has explored indicators which are perceived to be synonymous with healthy stock fundamentals. \citet{Penman2018} suggest that the Book-to-Price ratio (B/P) is misleading. Low B/P values reflect uncertainty about future cash-flows rather than
"cheap" buying opportunities. Additionally, \citet{Altman2016} argue that, amongst others, profitability is time-varying and find low predictive ability for longer time-periods. Specifically, while they find that profitability ratios like Return on Assets (ROA) provide efficient accuracy for a short horizon of two years , these measures fail to be consistent predictors over a ten-year horizon. They observe that in multivariate models, profitability is rendered largely insignificant when tested against solvency measures, such as the Equity Ratio, which dominates the prediction of distress irrespective of the horizon length.

  \item \textbf{Predictive Ability:} Traditional bankruptcy models heavily relied on linear combinations of ratios, like the Z-score introduced by \citep{Altman1968}. Since then, a variety of
risk-management models have been introduced, culminating in modern ML applications, such as
Random Forest (RF) or support vector machines \citep{Barboza2017}. Even though logit/probit models
work reasonably well, \citet{Jones2017} also recommends the use of more advanced machine-learning
methods, like AdaBoost or RF. \\
  
\item \textbf{Misalignment of Prediction Horizons and Objectives:} 
While traditional bankruptcy models aim to minimize credit risk, they often fail to minimize market risk. A fundamental disconnect exists between \textit{legal insolvency} and \textit{market implosion}.

\begin{itemize}
    \item \textbf{Lagging Indicators:} Legal bankruptcy is frequently the final stage of a long deterioration process. By the time a traditional Altman Z-Score or structural model flags a company as distressed, the market has often already priced in the failure, resulting in a "Zombie state" where the asset lingers at depressed valuations \citep{Tewari2024}. For an equity investor, the capital is lost at the \textit{implosion} event, not the bankruptcy filing.
    \item \textbf{The Cost of False Positives (Type I Errors):} In the context of equity indexing, the cost of a False Positive is mainly the opportunity cost. As noted in the "Agony and Ecstasy" framework, index returns are driven by a small tail of extreme winners. Traditional models, which penalize negative skewness too aggressively, risk flagging volatile but successful growth stocks as "distressed." Excluding a future "Ecstasy" stock (like NVIDIA) due to a conservative model, which produces too many False Positives, would severely underperform the benchmark, negating the benefits of avoiding the "Agony" stocks.
\end{itemize}

\end{enumerate}

\section{Research Gap and Proposed Methodology}

\subsection{Implications of Current Limitations}
 
\citet{Jabeur2021, Jabeur2023} have demonstrated that ML models, such as Gradient Boosting and Neural Networks,
outperform statistical models in predicting financial distress by capturing non-linear relationships between variables. \citet{Tewari2024} have built on these insights and applied modern ML-techniques to market-based risk-modelling, for which they find that XGBoost can predict up to 61\% of implosions in the test set
with a false positive rate of less than 3\%. \\

While the literature has identified a variety of ML-methods for credit-risk modelling, 
there is a lack of research on applying modern ML techniques to the specific problem of 
 \textit{market-based} catastrophic stock declines. Ensemble methods, like bagging and boosting,
 or Neural Networks are well suited to not only improve predictive accuracy, but also to 
 capture the time-series dynamics of individual stock risk. \\

% \begin{enumerate}
%     \item \textbf{From Ratios to Raw Data:} By utilizing raw financial data rather than derivative ratios, Machine Learning (ML) algorithms can autonomously learn complex, non-linear dependencies and interaction effects that traditional ratios (like Book-to-Price) fail to capture.
%     \item \textbf{From Static to Dynamic:} Financial distress is rarely a static state but a sequence of deteriorating signals. Traditional cross-sectional models ignore the temporal trajectory of firm fundamentals.
% \end{enumerate}

\subsection{Proposed Approach: The "Crash-Filtered" Index}

This thesis proposes bridging the gap between distress prediction and active index construction. While \citet{Tewari2024} established the concept of Catastrophic Stock Implosion (CSI), it remains unknown whether these insights can be operationalized into a viable risk-mitigation strategy. To address the "Agony and Ecstasy" dilemma, this research moves from "pure volatility forecasting" to "probabilistic implosion modeling" through the following three-stage methodology:

\begin{enumerate}
    \item \textbf{Model Fitting and Selection:} \\
    The research will model the probability of market-risk loss, specifically the CSI event, by adopting the definition established by \citet{Tewari2024}. The methodology extends current literature by moving beyond static snapshots of financial health. In addition to advanced ensemble methods (e.g., XGBoost, CatBoost) capable of handling non-linear interactions, this approach incorporates autoencoders to decode the noisy financial features.
    
% This extension allows the model to capture \textit{latent temporal dependencies} and the rate of change in fundamental deterioration, aiming to reduce the detection lag associated with traditional models.

    \item \textbf{Index Construction:} \\
    Based on the model outputs, a "Crash-Filtered" equity index will be constructed. This process involves a systematic re-weighting or exclusion mechanism that penalizes index constituents exhibiting a likelihood of implosion exceeding a calibrated threshold. The primary objective is to penalize identified "Agony" candidates (permanent capital loss or capital loss exceeding a threshold) while retaining the "Ecstasy" winners (usually featuring high volatility and returns). This distinction is critical to isolating the alpha generated purely by tail-risk mitigation, rather than by a generic low-beta factor tilt.

    \item \textbf{Backtesting and Performance Evaluation:} \\
    The efficacy of the strategy will be validated through out-of-sample backtesting using a rolling-forward cross-validation framework. The performance of the "Crash-Filtered" index will be benchmarked against:
    \begin{itemize}
        \item The unfiltered market-weighted benchmark (e.g., Russell 3000) to test for alpha generation.
        \item Traditional volatility-weighted portfolios (e.g., Minimum Volatility indices) to test for superior drawdown characteristics.
    \end{itemize}
    Evaluation metrics will prioritize risk-adjusted returns and tail-risk characteristics.
\end{enumerate}


%==== Chapter 2: Research Question ============================================%

\chapter{Research Question}

Based on the identified problem that standard metrics fail to distinguish between "recoverable volatility" and "permanent implosion" and that perceived quality-signals can be misleading, the following research question could be explored:

\section{Main Research Question}

\textit{To what extent does a 'Crash-Filtered' equity index, constructed via probabilistic implosion modeling, generate superior risk-adjusted returns compared to the market benchmark and traditional minimum-volatility strategies?}

% Volatility Consideration: You are correct—MinVol strategies do not explicitly claim high-volatility stocks will implode; they claim they are inefficient (the Low-Volatility Anomaly). Your thesis challenges this by arguing that indiscriminate volatility filtering is "dumb" because it cuts winners ("Ecstasy"). Comparing against MinVol is valid because you are testing if Smart Filtering (CSI) beats Indiscriminate Filtering (Variance).

\subsection{Sub-Question 1: Autoencoder}

Standard ratios (like P/E) are too noisy, but an Autoencoder can find the "hidden" structure of a failing firm.\\

\textit{Does the integration of latent features derived from Denoising Autoencoders significantly improve the predictive performance (Average Precision) of Ensemble models compared to those trained solely on raw financial data?} \\

\textit{Hypothesis:} Autoencoders will successfully denoise volatile market indicators, allowing the Ensemble model to identify "structural" distress earlier than models relying on raw accounting inputs.

\subsection{Sub-Question 2: The "False Positive" Advantage of Ensemble Methods}

Showing that decision trees (handling the non-linearities) are better than the linear Altman Z-Score at saving the "Ecstasy" stocks. \\

\textit{Do Ensemble methods exhibit a superior ability to distinguish between recoverable volatility ('Ecstasy') and terminal implosion ('Agony') compared to indiscriminate volatility-based exclusion strategies?} \\ 

\textit{Hypothesis:} Ensemble models will distinguish between 'good' volatility (growth) and 'bad' volatility (implosion) more effectively than linear models, thereby reducing the exclusion of high-performing winners.

\subsection{Sub-Question 3: Sensitivity}

\textit{Does shortening the prediction horizon from 12 months to 6 months significantly enhance the index's ability to react to distress signals, or does it introduce excessive turnover without performance gains?} \\ 

\textit{Prediction Horizon:} Reacting faster to changes.

%==== Chapter 3: Research Design ==============================================%

\chapter{Research Design}
\label{ch:research_design}

\begin{enumerate}

 \item \textbf{Unsupervised Feature Extraction (Autoencoder)} \\
    To address the high dimensionality of the dataset (over 400 features, including TSFEL time-series aggregations and fundamental ratios like \texttt{ff\_earn\_yld} and \texttt{ff\_cf\_sales}), an Undercomplete Denoising Autoencoder is employed. 
    
    The Autoencoder serves two distinct functions in the predictive pipeline:
    \begin{enumerate}
        \item \textbf{Dimensionality Reduction:} It compresses the noisy 400-dimensional input vector $x$ into a lower-dimensional latent representation vector $z$ (bottleneck layer). This extracts non-linear structural dependencies between variables (e.g., the interaction between decreasing Cash Flow from Operations and volatile Log Returns) that linear PCA would miss.
        \item \textbf{Anomaly Detection Feature:} Following the future work suggested by \citet{Tewari2024}, the model calculates the \textit{Reconstruction Error} ($\mathcal{L} = ||x - \hat{x}||^2$). A high reconstruction error serves as a proxy for "abnormal" market behavior, added as an explicit feature to the supervised Ensemble model.
    \end{enumerate}
    
\item \textbf{Defining the target variable (Forward-Looking)} \\
    Consistent with the supervised learning framework of \citet{Tewari2024}, the model is trained to predict the onset of a Catastrophic Stock Implosion (CSI). 
    
    A stock is classified as a positive CSI case if it satisfies the following three conditions:
    \begin{enumerate}
        \item \textbf{Initial Crash:} The cumulative return drops below a threshold $C = -0.8$ (i.e., an 80\% drawdown from the trailing peak).
        \item \textbf{Zombie Period:} The stock enters a stagnation period of duration $T = 78$ weeks (approx. 1.5 years).
        \item \textbf{Non-Recovery:} Throughout the zombie period, the cumulative return never exceeds the recovery ceiling $M = -0.2$ (remaining at least 20\% below the peak).
    \end{enumerate}

    While \citet{Tewari2024} employ a yearly prediction horizon ($h=12$ months), this study follows this approach and also tests
    whether this horizon can be relaxed to  $h=6$ months to enhance the temporal precision required for more frequent index rebalancing.
    The binary target variable $y_{i,t}$ is defined as:
    
    \begin{equation}
        y_{i,t} =
        \begin{cases}
        1 & \text{if stock } i \text{ triggers } C=-0.8 \text{ within } [t, t+h] \text{ and satisfies the zombie criteria} \\
        0 & \text{otherwise}
        \end{cases}
    \end{equation}

    \textbf{Sensitivity Analysis and Parameter Relaxation:} \\
    The baseline definition of a 1.5-year ($T=78$ weeks) zombie period is highly conservative. To assess robustness, this study will perform a sensitivity analysis by relaxing the duration parameter $T$ to \textbf{less than 1 year} (e.g., $T=26$ or $T=52$ weeks). This tests whether the "Agony" of implosion can be effectively captured with a shorter confirmation window, potentially allowing the model to identify distress signals earlier without significantly increasing the False Positive Rate.

\item \textbf{Model Development} \\
  \begin{itemize}
    \item \textbf{Algorithm:} Ensemble methods (XGBoost, CatBoost) are used to predict the probability of CSI ($y_{i,t}=1$).
    \item \textbf{Validation of Subquestion 1:} To isolate the contribution of the Autoencoder, an internal comparison will be conducted between:
        \begin{itemize}
            \item \textit{Model A:} Gradient Boosting on Raw Financial Ratios.
            \item \textit{Model B:} Gradient Boosting on Autoencoder Latent Features.
        \end{itemize}
    \item \textbf{Validation of Subquestion 2:} The best-performing Ensemble model is compared against:
        \begin{itemize}
            \item The Naive Baseline (class prior).
            \item Logistic Regression.
            \item The \textbf{Inverted Altman Z-Score}, treated as a continuous risk ranking to allow for direct AUC-PR comparison.
        \end{itemize}
  \end{itemize}
  
  \item \textbf{Index Construction} \\
   The "Crash-Filtered" index is constructed by filtering the CRISP-universe.
  \begin{itemize}
    \item \textbf{Rebalancing:} The portfolio is rebalanced \textbf{anually} or \textbf{semi-anually} (dependent on the prediction horizon, e.g. $h=12$ months)
    to ensure timely removal of deteriorating assets.
    \item \textbf{Filtering Mechanism:} For each stock, if the predicted probability $\hat{p}_{i,t} > \theta$, it is excluded.
    \item \textbf{Calibration:} $\theta$ is calibrated on the validation set to maximize the F1-score (harmonic mean of Precision and Recall), 
    balancing the cost of False Negatives (holding an implosion) against False Positives (missing a winner).
  \end{itemize}
  
    \item \textbf{Performance Evaluation} \\
    The strategy is backtested using a rolling-forward cross-validation framework. Performance is benchmarked against the \textbf{Market-Weighted Index} (unfiltered) and the \textbf{MSCI-USA Minimum Volatility Index}.
  
\end{enumerate} 

%==== Chapter 4: Expected Contribution ========================================%

\chapter{Expected Contribution}

\begin{enumerate}

\item \textbf{Methodological Advancement: A Hybrid Unsupervised-Supervised Framework} \\
    While \citet{Tewari2024} successfully demonstrated the efficacy of supervised learning (XGBoost) for predicting catastrophic implosions, they explicitly identified unsupervised learning as a "promising avenue for research" to model implosions as anomalies. 
    This thesis directly addresses this call by introducing a \textbf{Hybrid Autoencoder-Ensemble architecture}. By using an Autoencoder to denoise high-dimensional financial data and extract latent "distress structures," this research contributes a novel feature engineering pipeline that aims to improve predictive performance in noisy market regimes where standard ratios fail.

    \item \textbf{Practical Operationalization: From Prediction to Investment Utility} \\
    Existing literature often focuses on the statistical accuracy of distress models (e.g., AUC, Precision, Recall). However, a statistical prediction does not guarantee economic value. 
    This thesis shifts the focus from "pure prediction" to "portfolio construction." By developing a systematic \textbf{"Crash-Filtered" Index}, this research validates whether the statistical signal of a Catastrophic Stock Implosion (CSI) can be monetized. It provides empirical evidence on whether avoiding specific "Agony" events generates superior risk-adjusted returns compared to generic Minimum Volatility strategies, thus bridging the gap between machine learning metrics and investor outcomes.

    \item \textbf{Theoretical Distinction: Disentangling Volatility from Implosion} \\
    Traditional risk models treat volatility as a proxy for risk, often penalizing high-growth "Ecstasy" stocks that exhibit "good" volatility. 
    This thesis contributes to the "Agony and Ecstasy" framework (Cembalest, 2014) by empirically testing whether "Catastrophic Implosion" is a distinct risk factor separate from standard price variance. By calibrating the model to minimize Type I errors (False Positives), this research seeks to demonstrate that it is possible to decouple downside tail-risk protection from the upside participation of growth stocks—a distinction that standard low-beta factors fail to achieve.

\end{enumerate}

%==== Chapter 5: Open Questions / Refinements =================================%

\chapter{Refinements}

%% Table 1: Split of firms (implosion vs non implosion) by geometric return.

\begin{table}[!h]
\centering
\caption{Long-Term Growth Distribution: Imploded vs. Non-Imploded Firms}
\centering
\begin{tabular}[t]{lrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Cohort Distribution} \\
\cmidrule(l{3pt}r{3pt}){2-3}
Growth\_Category & Imploded Firms & Never Imploded\\
\midrule
\cellcolor{gray!10}{High Growth (>10\%)} & \cellcolor{gray!10}{4.9\%} & \cellcolor{gray!10}{40.5\%}\\
Moderate Growth (5-10\%) & 9.1\% & 25.9\%\\
\cellcolor{gray!10}{Low Growth (0-5\%)} & \cellcolor{gray!10}{13.8\%} & \cellcolor{gray!10}{13.7\%}\\
Low Losses (0 to -5\%) & 13.0\% & 7.0\%\\
\cellcolor{gray!10}{High Losses (<-5\%)} & \cellcolor{gray!10}{59.1\%} & \cellcolor{gray!10}{12.9\%}\\
\bottomrule
\end{tabular}
\end{table}

%% Table 2: Same as 1 with split by no. of CSI events.
\begin{table}[!h]
\centering
\caption{Long-Term Growth Distribution by Frequency of Implosion Events}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{8}{c}{Number of CSI Events Experienced} \\
\cmidrule(l{3pt}r{3pt}){2-9}
Growth\_Category & Events: 0 & Events: 1 & Events: 2 & Events: 3 & Events: 4 & Events: 5 & Events: 6 & Events: 7\\
\midrule
\cellcolor{gray!10}{High Growth (>10\%)} & \cellcolor{gray!10}{40.5\%} & \cellcolor{gray!10}{7.4\%} & \cellcolor{gray!10}{3.1\%} & \cellcolor{gray!10}{3.0\%} & \cellcolor{gray!10}{1.1\%} & \cellcolor{gray!10}{0.0\%} & \cellcolor{gray!10}{5.6\%} & \cellcolor{gray!10}{0.0\%}\\
Moderate Growth (5-10\%) & 25.9\% & 10.8\% & 8.6\% & 7.4\% & 9.7\% & 2.0\% & 0.0\% & 0.0\%\\
\cellcolor{gray!10}{Low Growth (0-5\%)} & \cellcolor{gray!10}{13.7\%} & \cellcolor{gray!10}{13.6\%} & \cellcolor{gray!10}{16.5\%} & \cellcolor{gray!10}{11.9\%} & \cellcolor{gray!10}{14.0\%} & \cellcolor{gray!10}{9.8\%} & \cellcolor{gray!10}{11.1\%} & \cellcolor{gray!10}{0.0\%}\\
Low Losses (0 to -5\%) & 7.0\% & 10.1\% & 12.5\% & 19.3\% & 19.4\% & 15.7\% & 22.2\% & 14.3\%\\
\cellcolor{gray!10}{High Losses (<-5\%)} & \cellcolor{gray!10}{12.9\%} & \cellcolor{gray!10}{58.1\%} & \cellcolor{gray!10}{59.2\%} & \cellcolor{gray!10}{58.5\%} & \cellcolor{gray!10}{55.9\%} & \cellcolor{gray!10}{72.5\%} & \cellcolor{gray!10}{61.1\%} & \cellcolor{gray!10}{85.7\%}\\
\bottomrule
\end{tabular}}
\end{table}

%% Table 3: Split of firms (implosion vs non implosion) by geometric return and min/max annual return.
\begin{table}[!h]
\centering
% \caption{}
\fontsize{7pt}{9pt}\selectfont 
% -------------------------
\begin{tabular}[t]{lrr}
\toprule
\multicolumn{1}{c}{Category} & \multicolumn{2}{c}{Cohort Distribution} \\
\cmidrule(l{2pt}r{3pt}){2-3}
& Imploded Firms & Never Imploded\\
\midrule
\cellcolor{gray!10}{High Growth (>10\%)} & \cellcolor{gray!10}{4.9\%} & \cellcolor{gray!10}{40.5\%}\\
Moderate Growth (5-10\%) & 9.1\% & 25.9\%\\
\cellcolor{gray!10}{Low Growth (2-5\%)} & \cellcolor{gray!10}{8.2\%} & \cellcolor{gray!10}{9.0\%}\\
Stagnation (-2-2\%) & 0.0\% & 0.1\%\\
\cellcolor{gray!10}{Value Destruction (<-2\%)} & \cellcolor{gray!10}{64.8\%} & \cellcolor{gray!10}{15.4\%}\\
\addlinespace
Value Destruction (No Recovery) & 2.2\% & 0.8\%\\
\cellcolor{gray!10}{Unknown} & \cellcolor{gray!10}{10.8\%} & \cellcolor{gray!10}{8.4\%}\\
\bottomrule
\multicolumn{3}{l}{\scriptsize \textit{Note:} Categories are measured by the average geometric return.} \\
\end{tabular}
\end{table}

%% Table 4: Same as Table 3 with split by no. of CSI events.
\begin{table}[!h]
\centering
\caption{Long-Term Growth Distribution by Frequency of Implosion Events}
\centering
\resizebox{\ifdim\width>\linewidth\linewidth\else\width\fi}{!}{
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{8}{c}{Number of CSI Events Experienced} \\
\cmidrule(l{3pt}r{3pt}){2-9}
Growth\_Category & Events: 0 & Events: 1 & Events: 2 & Events: 3 & Events: 4 & Events: 5 & Events: 6 & Events: 7\\
\midrule
\cellcolor{gray!10}{High Growth (>10\%)} & \cellcolor{gray!10}{40.5\%} & \cellcolor{gray!10}{7.4\%} & \cellcolor{gray!10}{3.1\%} & \cellcolor{gray!10}{3.0\%} & \cellcolor{gray!10}{1.1\%} & \cellcolor{gray!10}{0.0\%} & \cellcolor{gray!10}{5.6\%} & \cellcolor{gray!10}{0.0\%}\\
Moderate Growth (5-10\%) & 25.9\% & 10.8\% & 8.6\% & 7.4\% & 9.7\% & 2.0\% & 0.0\% & 0.0\%\\
\cellcolor{gray!10}{Low Growth (2-5\%)} & \cellcolor{gray!10}{9.0\%} & \cellcolor{gray!10}{8.4\%} & \cellcolor{gray!10}{8.6\%} & \cellcolor{gray!10}{8.1\%} & \cellcolor{gray!10}{8.6\%} & \cellcolor{gray!10}{3.9\%} & \cellcolor{gray!10}{11.1\%} & \cellcolor{gray!10}{0.0\%}\\
Stagnation (-2-2\%) & 0.1\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\%\\
\cellcolor{gray!10}{Value Destruction (<-2\%)} & \cellcolor{gray!10}{15.4\%} & \cellcolor{gray!10}{60.3\%} & \cellcolor{gray!10}{65.9\%} & \cellcolor{gray!10}{68.9\%} & \cellcolor{gray!10}{66.7\%} & \cellcolor{gray!10}{86.3\%} & \cellcolor{gray!10}{72.2\%} & \cellcolor{gray!10}{85.7\%}\\
\addlinespace
Value Destruction (No Recovery) & 0.8\% & 3.5\% & 2.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\%\\
\cellcolor{gray!10}{Unknown} & \cellcolor{gray!10}{8.4\%} & \cellcolor{gray!10}{9.5\%} & \cellcolor{gray!10}{11.8\%} & \cellcolor{gray!10}{12.6\%} & \cellcolor{gray!10}{14.0\%} & \cellcolor{gray!10}{7.8\%} & \cellcolor{gray!10}{11.1\%} & \cellcolor{gray!10}{14.3\%}\\
\bottomrule
\end{tabular}}
\end{table}

%% Continue with text.

\begin{enumerate}

\item \textbf{Validation of the "Avoidance Alpha" Hypothesis} \\

The analysis confirms that the CSI signal successfully isolates "toxic" assets. In the "Imploded" cohort, \textbf{48.5\%} of firms resulted in long-term \textit{Value Destruction} (CAGR $<-2\%$), compared to only \textbf{16.8\%} in the "Clean" cohort.

This 3x increase in the probability of value destruction validates the conservative definition of the zombie period ($T=78$ weeks). It demonstrates that the target variable $y=1$ is not capturing noise, but rather a fundamental deterioration in solvency. Consequently, the primary mechanism of the "Crash-Filtered" Index is empirically justified: the outperformance is expected to stem primarily from the \textit{avoidance of zeros} (insolvency) rather than the selection of winners.

\item \textbf{Quantifying the "Phoenix Cost" (Type I Error Trade-off)} \\
A critical refinement for the rebalancing strategy arises from the finding that \textbf{12.5\%} of Imploded firms eventually became "High Growth" assets ($>10\%$ CAGR). These are "Phoenix" stocks that suffered a catastrophic drawdown but successfully restructured (e.g., Amazon in 2001).



This finding highlights a distinct "Phoenix Cost"—the opportunity cost of excluding a turnaround winner. To mitigate this, the Research Design (Chapter \ref{ch:research_design}) must explicitly incorporate a \textbf{Re-entry Mechanism}. The index cannot permanently blacklist a firm following a CSI event. Instead, the annual or semi-annual rebalancing frequency ($h=6$ or $12$ months) is essential to allow the model to reassess the risk. Once the Autoencoder's anomaly signal decays, the strategy must permit these "Phoenix" stocks to re-enter the portfolio to capture the post-distress recovery phase.

\item \textbf{The "Serial Offender" Non-Linearity} \\
The breakdown of outcomes by event count reveals a non-linear dose-response relationship. Firms experiencing exactly one CSI event show a \textbf{47.7\%} rate of value destruction, which rises to \textbf{53.1\%} for firms with two events. 

This refines the feature engineering approach for the Supervised Model. It suggests that "Past CSI Count" or "Time Since Last Implosion" are not merely historical metadata but predictive state variables. A linear model might interpret a -80\% crash as a "mean reversion" buying opportunity; however, the data suggests that conditional on crashing once, the probability of future stagnation remains structurally elevated. This justifies the use of the \textbf{Autoencoder}, which is capable of encoding this non-linear "memory" of distress into the latent feature vector $z$, differentiating between a "cheap" value stock and a "structurally impaired" serial offender.

\end{enumerate}

%==== Chapter 6: Data =========================================================%

\chapter{Data}

%======= Section 6A ===========#

\section{Constituent Universe (CRSP)}

The study utilizes the Center for Research in Security Prices (CRSP) database, specifically the \textit{Stock Security Information History} (\texttt{stksecurityinfohist}) and \textit{Monthly Stock File} (\texttt{msf}) accessed via WRDS. To ensure data quality and constituent continuity, the investable universe is constructed through a four-step process:

\begin{enumerate}
    \item \textbf{Whole Sample:} The initial sample includes all unique permanent identifiers (\texttt{permno}) in the CRSP database, covering the entire history of US-listed securities to eliminate survivorship bias.
    \item \textbf{Aggregation:} Data is aggregated by \texttt{permno} to define the distinct trading lifetime, calculated as the duration between the earliest listing date and the final removal date.
    \item \textbf{Filtering Criteria:} The universe is subjected to the following filters:
    \begin{itemize}
        \item \textbf{Security Type:} Restricted to common equities (\texttt{securitytype="EQTY"}, \texttt{sharetype="COM"}), explicitly excluding ETFs and other Mutual Funds.
        \item \textbf{Exchange:} Limited to major US exchanges (NYSE, AMEX, NASDAQ).
        \item \textbf{Historical Depth:} Excludes securities removed prior to January 1, 1966.
        \item \textbf{Minimum Lifetime:} Requires a listing duration of at least 5 years to learn on multiple datapoints from each firm and to ensure the same methodology for the computation of the dependent variable.
    \end{itemize}
\end{enumerate}

\subsection{Sample Overview}

The final dataset spans from January 1960 to December 2024, comprising 1,064,324 firm-month observations across 3,629 unique equity issuers (Table \ref{tab:agg_stats}).

\begin{table}[!h]
\centering
\caption{Aggregate Sample Statistics}
\centering
\begin{tabular}[t]{lr}
\toprule
Statistic & Value\\
\midrule
Total Unique Firms (N) & 3,263\\
Total Observations & 51,773\\
Average Years per Firm & 14\\
Sample Start Date & 1998-12-31\\
Sample End Date & 2024-12-31\\
\addlinespace
Total Identified Implosions & 2,236\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Industry Classification}

Economic activity is categorized using the \textbf{North American Industry Classification System (NAICS)}. NAICS codes are available in CRSP from August 2001; for securities delisted prior to this, legacy SIC codes are mapped to equivalent NAICS sectors to ensure complete coverage. These classifications will later on by changed
to match the ones by the Global Industry Classification Standard (GICS).

\begin{table}[!h]
\centering
\caption{Constituent Distribution by Industry Sector (NAICS/SIC Hybrid)}
\centering
\begin{tabular}[t]{lrrl}
\toprule
Industry Sector & Count & Percentage (\%) & Source\\
\midrule
Manufacturing & 1285 & 35.4 & NAICS\\
Finance and Insurance & 547 & 15.1 & NAICS\\
Professional, Scientific, \& Technical Services & 237 & 6.5 & NAICS\\
Real Estate and Rental and Leasing & 217 & 6.0 & NAICS\\
Information & 215 & 5.9 & NAICS\\
\addlinespace
Mining, Quarrying, and Oil \& Gas Extraction & 188 & 5.2 & NAICS\\
Admin. Support \& Waste Mgmt & 139 & 3.8 & NAICS\\
Retail Trade & 131 & 3.6 & NAICS\\
Wholesale Trade & 128 & 3.5 & NAICS\\
Management of Companies and Enterprises & 108 & 3.0 & NAICS\\
\addlinespace
Transportation and Warehousing & 102 & 2.8 & NAICS\\
Utilities & 87 & 2.4 & NAICS\\
Accommodation and Food Services & 67 & 1.8 & NAICS\\
Health Care and Social Assistance & 57 & 1.6 & NAICS\\
Construction & 55 & 1.5 & NAICS\\
\addlinespace
Educational Services & 18 & 0.5 & NAICS\\
Arts, Entertainment, and Recreation & 16 & 0.4 & NAICS\\
Agriculture, Forestry, Fishing \& Hunting & 15 & 0.4 & NAICS\\
Other Services (except Public Admin) & 14 & 0.4 & NAICS\\
Public Administration & 2 & 0.1 & NAICS\\
\bottomrule
\end{tabular}
\end{table}

%======= Section 6B ===========#

\section{Features (Compustat)}

To capture corporate fundamental signals, we utilize the \textbf{Compustat Fundamentals Annual} database (\texttt{comp.funda}) accessed via WRDS. This dataset provides comprehensive balance sheet, income statement, and cash flow data for North American publicly traded companies.

\subsection{Data Linking and Merging}

To align fundamental data with the market data defined in the previous section, we employ the \textbf{CRSP/Compustat Merged (CCM)} linking table (\texttt{crsp.ccmxpf\_lnkhist}). This ensures accurate mapping between CRSP permanent identifiers (\texttt{permno}) and Compustat identifiers (\texttt{gvkey}). The linking process enforces the following strict validity criteria:
\begin{itemize}
    \item \textbf{Link Type:} Restricted to \texttt{LC} (Link Research), \texttt{LU} (Unresearched Link), and \texttt{LS} (Link System) to ensure high-confidence matches.
    \item \textbf{Link Primary Marker:} Limited to \texttt{P} (Primary) and \texttt{C} (Composite) to isolate the primary trading vehicle for the firm.
    \item \textbf{Date Validity:} Fundamental observations are only retained if the fiscal year-end date falls within the valid range defined by the link start (\texttt{linkdt}) and end dates (\texttt{linkenddt}).
\end{itemize}

\subsection{Filtering and Universe Alignment}

Within the Compustat universe, we apply standard filters to ensure data consistency and comparability across firms. We restrict the sample to the Industrial Format (\texttt{indfmt="INDL"}) and Standardized Data Format (\texttt{datafmt="STD"}). Furthermore, we utilize Consolidated financial statements (\texttt{consol="C"}) to capture the full economic entity of the firm.

\subsection{Look-Ahead Bias Mitigation}

TBD.

% A critical aspect of the data processing pipeline is the prevention of look-ahead bias. Financial data is indexed by its fiscal year-end date (\texttt{datadate}) in Compustat, but this information is not publicly available to investors on that specific date. 
% 
% To accurately simulate the information set available to a market participant, we institute a **reporting lag protocol**. We define the \textit{Public Availability Date} as 4 months after the fiscal year-end:
% \begin{equation}
%     \text{Public Date}_t = \text{Datadate}_t + 4 \text{ months}
% \end{equation}
% This conservative 4-month lag (approx. 120 days) accounts for the statutory 90-day filing deadline for 10-K reports plus a buffer for processing delays, ensuring that the model only trains on data that would have been strictly observable at the time of prediction.

\subsection{Feature Selection}

We extract a comprehensive set of fundamental variables capturing distinct dimensions of firm health. Beyond standard performance metrics, our selection specifically targets **distress precursors** identified in the "Zombie" firm literature. These include \textit{Retained Earnings} (to capture historical accumulated deficits), \textit{Deferred Tax Assets} (proxying for loss carryforwards), and \textit{Discretionary Expenses} (advertising and R\&D cuts signaling cash preservation).

The feature set is organized into three primary categories: Balance Sheet Structure, Operational Performance, and Supplementary Distress Signals. The complete list of variables is detailed in Table \ref{tab:compustat_vars}.

\begin{table}[ht]
\centering
\caption{Selected Fundamental Features (Compustat)}
\label{tab:compustat_vars}
\scriptsize
\begin{tabular}{llp{7.5cm}}
\toprule
\textbf{Category} & \textbf{Variable Code} & \textbf{Description} \\
\midrule
\textbf{Balance Sheet: Assets} 
 & \texttt{at} / \texttt{act} & Total Assets / Total Current Assets \\
 & \texttt{che} / \texttt{ivst} & Cash \& Short-Term Inv. / Short-Term Investments \\
 & \texttt{rect} / \texttt{invt} & Receivables (Channel stuffing risk) / Inventories \\
 & \texttt{wcap} & Working Capital (Liquidity buffer) \\
 & \texttt{ppent} / \texttt{intan} & Net PP\&E / Intangibles (Soft assets) \\
 & \texttt{gdwl} & Goodwill (Impairment risk) \\
 & \texttt{txdba} & Deferred Tax Asset (Long Term) - \textit{Proxy for NOLs} \\
\addlinespace
\textbf{Balance Sheet: Liab/Eq} 
 & \texttt{lt} / \texttt{lct} & Total Liabilities / Total Current Liabilities \\
 & \texttt{dltt} / \texttt{dlc} & Long-Term Debt / Debt in Current Liabilities \\
 & \texttt{dd1} & Long-Term Debt Due in 1 Year (\textit{Refinancing wall}) \\
 & \texttt{ap} / \texttt{txp} & Accounts Payable / Income Taxes Payable \\
 & \texttt{txditc} & Deferred Taxes \& Inv. Tax Credit (Non-current) \\
 & \texttt{seq} / \texttt{re} & Stockholders' Equity / Retained Earnings (\textit{Accum. Deficit}) \\
 & \texttt{pstk} / \texttt{mib} & Preferred Stock / Noncontrolling Interest \\
 & \texttt{tstk} & Treasury Stock (Contra-equity) \\
\addlinespace
\textbf{Income Statement} 
 & \texttt{sale} / \texttt{gp} & Net Sales / Gross Profit \\
 & \texttt{cogs} / \texttt{xsga} & Cost of Goods Sold / SG\&A Expense \\
 & \texttt{ebit} / \texttt{oibdp} & EBIT / Operating Income Before Deprec. (\textit{EBITDA}) \\
 & \texttt{ni} / \texttt{pi} & Net Income / Pretax Income (\textit{Tax efficiency check}) \\
 & \texttt{epsfi} & Earnings Per Share (Basic) \\
 & \texttt{spi} & Special Items (\textit{Early warning signal}) \\
 & \texttt{xint} / \texttt{xinst} & Total Interest Exp. / Short-Term Interest Exp. \\
 & \texttt{xrd} / \texttt{xad} & R\&D Exp. / Advertising Exp. (\textit{Discretionary cuts}) \\
 & \texttt{xrent} & Rental Expense (Operational rigidity) \\
 & \texttt{dp} & Depreciation \& Amortization \\
\addlinespace
\textbf{Supplementary} 
 & \texttt{emp} & Number of Employees (\textit{Real economy anchor}) \\
 & \texttt{dpr} / \texttt{ppegt} & Accum. Depreciation / Gross PPE (\textit{Asset age ratio}) \\
 & \texttt{fca} & Foreign Exchange Income/Loss \\
 & \texttt{mkvalt} & Market Value (Fiscal Year-End) \\
\bottomrule
\end{tabular}
\end{table}

%======= Section 6C ===========#

\section{Dataset Merging}

%==== Chapter 7: Descriptive Statistics =======================================%

\chapter{Descriptive Statistics}

%======= Section 7A ===========#

\section{Dataset}
\subsection{Sample Statistic}

\begin{table}[!h]
\centering
\caption{Aggregate Sample Statistics}
\centering
\begin{tabular}[t]{lr}
\toprule
Statistic & Value\\
\midrule
Total Unique Firms (N) & 3,263\\
Total Observations & 51,773\\
Average Years per Firm & 14\\
Sample Start Date & 1998-12-31\\
Sample End Date & 2024-12-31\\
\addlinespace
Total Identified Implosions & 2,236\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]
\centering
\caption{Distribution of Catastrophic Stock Implosion (CSI) Events per Firm}
\centering
\begin{tabular}[t]{crr}
\toprule
Total CSI Events & Number of Firms & Percentage (\%)\\
\midrule
\cellcolor{gray!10}{0} & \cellcolor{gray!10}{2167} & \cellcolor{gray!10}{66.41}\\
1 & 537 & 16.46\\
\cellcolor{gray!10}{2} & \cellcolor{gray!10}{255} & \cellcolor{gray!10}{7.81}\\
3 & 135 & 4.14\\
\cellcolor{gray!10}{4} & \cellcolor{gray!10}{93} & \cellcolor{gray!10}{2.85}\\
\addlinespace
5 & 51 & 1.56\\
\cellcolor{gray!10}{6} & \cellcolor{gray!10}{18} & \cellcolor{gray!10}{0.55}\\
7 & 7 & 0.21\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]
\centering
\caption{Temporal Distribution of Catastrophic Stock Implosions (Target Variable y)}
\centering
\begin{tabular}[t]{crr}
\toprule
Year & Total Events & Percentage (\%)\\
\midrule
\cellcolor{gray!10}{1998} & \cellcolor{gray!10}{83} & \cellcolor{gray!10}{3.71}\\
1999 & 56 & 2.50\\
\cellcolor{gray!10}{2000} & \cellcolor{gray!10}{124} & \cellcolor{gray!10}{5.55}\\
2001 & 16 & 0.72\\
\cellcolor{gray!10}{2002} & \cellcolor{gray!10}{57} & \cellcolor{gray!10}{2.55}\\
\addlinespace
2003 & 44 & 1.97\\
\cellcolor{gray!10}{2004} & \cellcolor{gray!10}{22} & \cellcolor{gray!10}{0.98}\\
2005 & 60 & 2.68\\
\cellcolor{gray!10}{2006} & \cellcolor{gray!10}{69} & \cellcolor{gray!10}{3.09}\\
2007 & 68 & 3.04\\
\addlinespace
\cellcolor{gray!10}{2008} & \cellcolor{gray!10}{55} & \cellcolor{gray!10}{2.46}\\
2009 & 114 & 5.10\\
\cellcolor{gray!10}{2010} & \cellcolor{gray!10}{70} & \cellcolor{gray!10}{3.13}\\
2011 & 36 & 1.61\\
\cellcolor{gray!10}{2012} & \cellcolor{gray!10}{80} & \cellcolor{gray!10}{3.58}\\
\addlinespace
2013 & 86 & 3.85\\
\cellcolor{gray!10}{2014} & \cellcolor{gray!10}{81} & \cellcolor{gray!10}{3.62}\\
2015 & 95 & 4.25\\
\cellcolor{gray!10}{2016} & \cellcolor{gray!10}{134} & \cellcolor{gray!10}{5.99}\\
2017 & 155 & 6.93\\
\addlinespace
\cellcolor{gray!10}{2018} & \cellcolor{gray!10}{56} & \cellcolor{gray!10}{2.50}\\
2019 & 185 & 8.27\\
\cellcolor{gray!10}{2020} & \cellcolor{gray!10}{182} & \cellcolor{gray!10}{8.14}\\
2021 & 211 & 9.44\\
\cellcolor{gray!10}{2022} & \cellcolor{gray!10}{97} & \cellcolor{gray!10}{4.34}\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Industry Distribution}

\begin{table}[!h]
\centering
\caption{Constituent Distribution by Industry Sector (NAICS/SIC Hybrid)}
\centering
\begin{tabular}[t]{lrrl}
\toprule
Industry Sector & Count & Percentage (\%) & Source\\
\midrule
Manufacturing & 1169 & 35.9 & NAICS\\
Finance and Insurance & 510 & 15.7 & NAICS\\
Real Estate and Rental and Leasing & 214 & 6.6 & NAICS\\
Professional, Scientific, \& Technical Services & 191 & 5.9 & NAICS\\
Information & 188 & 5.8 & NAICS\\
\addlinespace
Mining, Quarrying, and Oil \& Gas Extraction & 156 & 4.8 & NAICS\\
Admin. Support \& Waste Mgmt & 127 & 3.9 & NAICS\\
Retail Trade & 126 & 3.9 & NAICS\\
Wholesale Trade & 113 & 3.5 & NAICS\\
Transportation and Warehousing & 96 & 2.9 & NAICS\\
\addlinespace
Utilities & 72 & 2.2 & NAICS\\
Management of Companies and Enterprises & 69 & 2.1 & NAICS\\
Accommodation and Food Services & 62 & 1.9 & NAICS\\
Health Care and Social Assistance & 56 & 1.7 & NAICS\\
Construction & 53 & 1.6 & NAICS\\
\addlinespace
Arts, Entertainment, and Recreation & 14 & 0.4 & NAICS\\
Other Services (except Public Admin) & 13 & 0.4 & NAICS\\
Agriculture, Forestry, Fishing \& Hunting & 12 & 0.4 & NAICS\\
Educational Services & 12 & 0.4 & NAICS\\
Public Administration & 1 & 0.0 & NAICS\\
\addlinespace
NA & 1 & 0.0 & SIC (Proxy)\\
\bottomrule
\end{tabular}
\end{table}

%======= Section 7A ===========#


%==== Chapter 8: Descriptive Statistics =======================================%

\chapter{Objective Function}

\section{Model Selection and Optimization Strategy}

\subsection{The Economic Case for Recall at Fixed False Positive Rates}
The selection of an appropriate objective function is critical when bridging the gap between statistical probability and investment utility. While standard classification metrics like Accuracy or F1-Score are ubiquitous, they often fail to capture the asymmetric costs inherent in the "Agony and Ecstasy" framework. In the context of constructing a "Crash-Filtered" Index, the cost of a False Positive (Type I Error) is not merely a statistical nuisance but a substantial opportunity cost: identifying a high-growth "Ecstasy" stock (e.g., NVIDIA) as a crash candidate results in its exclusion, thereby generating underperformance relative to the benchmark.

Consequently, this thesis adopts a two-staged evaluation strategy:

\begin{enumerate}
    \item \textbf{Hyperparameter Optimization (CV): Average Precision (AP).} 
    During the cross-validation and training phase, we optimize for the Area Under the Precision-Recall Curve (AUC-PR/AP). Unlike the Receiver Operating Characteristic (ROC), AP focuses exclusively on the performance of the minority class (Catastrophic Stock Implosion). Since the dataset is highly imbalanced ($\approx 5\%$ prevalence), AP provides a more robust signal for tuning hyperparameters (e.g., tree depth, learning rate) without committing to a specific decision threshold, ensuring the model effectively ranks distress probabilities.

    \item \textbf{Final Model Selection: Recall at Fixed FPR.} 
    For the final economic evaluation, we transition to a constraint-based metric: \textbf{Recall at Fixed False Positive Rate (FPR)}. This metric aligns directly with the practical "Risk Budget" of a portfolio manager. By fixing the FPR at conservative levels (e.g., $3\%$, $5\%$, or $10\%$), we effectively set a "budget" for mistakenly excluding healthy firms. The objective is to maximize the number of "Agony" stocks identified (Recall) while strictly capping the exclusion of potential "Ecstasy" winners (FPR). This approach empirically tests whether the model can decouple tail-risk protection from upside participation, a key limitation of traditional Minimum Volatility strategies.
\end{enumerate}

%======= Section 2: The AUC Trap ===========%

\section{The Potential Pitfall of AUC-ROC}

\subsection{The "False Positive" Trap in Imbalanced Datasets}
While the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) is the standard metric in bankruptcy prediction literature, it presents a potential "trap" for this specific research design. The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate across all thresholds. In a dataset where negatives (non-imploding firms) comprise over $95\%$ of observations, the FPR denominator is large. Consequently, a model can misclassify a significant number of high-volatility "Ecstasy" stocks as implosions (False Positives) without significantly increasing the calculated FPR, thereby inflating the AUC-ROC score.

This phenomenon is particularly dangerous for "Avoidance Alpha" strategies. A high AUC-ROC might reflect a model that is excellent at classifying obvious, low-volatility "safe" stocks (the easy negatives) but fails to distinguish between high-volatility winners and high-volatility losers in the tail. 

Therefore, while this study will report AUC-ROC for comparability with existing literature (e.g., \citet{Altman1968}), it serves as a secondary diagnostic. We explicitly monitor for divergence between AUC-ROC and Average Precision to ensure the model is not achieving high scores simply by overfitting to the majority class of "safe" firms while failing to solve the central "Agony and Ecstasy" dilemma.




%==== Chapter 9: References ===================================================%

\bibliographystyle{plainnat} 
\bibliography{references}

%==== Appendix ================================================================%

% \appendix 
% 
% \chapter{Overview}

%==== END =====================================================================%

\end{document}
